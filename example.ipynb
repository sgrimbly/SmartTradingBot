{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python373jvsc74a57bd05c59d13babb3785341d434f89bea72ce3341ca5629a070890467fed2fa96bf00",
   "display_name": "Python 3.7.3 64-bit ('base': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "## Main Trading Bot Logic"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "The first algorithm we will test out is DQN. This is the de facto standard for single agent RL algorithms at this point. \n",
    "\n",
    "<img src=\"DQN.png\" alt=\"drawing\" width=\"700\"/>"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Before we actually start working on the core algorithm we are going to use for the trading bot, we should probably make sure we can pull the appropriate data and clean it if necessary. Perhaps the most obvious place to start is [Yahoo! Finance](https://finance.yahoo.com/).\n",
    "\n",
    "We will set this up so we can run our algorithm with some input parameters like the ticker code for a stock/crypto and automate the cleaning and training process."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Test on LunarLander"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('LunarLander-v2')\n",
    "env.seed(0)\n",
    "print('State shape: ', env.observation_space.shape)\n",
    "print('Number of actions: ', env.action_space.n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# state_dim defines the number of days to take in a\n",
    "#TAU = 1e-3              # for soft update of target parameters\n",
    "lunar_agent = agent.DQNAgent(\n",
    "    state_dim=8,\n",
    "    action_dim=4,\n",
    "    hidden_layer_sizes=[64,64],\n",
    "    buffer_size=10000,\n",
    "    batch_size=64,\n",
    "    discount=0.99,\n",
    "    learning_rate=5e-4,\n",
    "    learning_freq=4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Evaluate untrained model\n",
    "\n",
    "state = env.reset()\n",
    "for j in range(200):\n",
    "    state = tf.reshape(state,shape=(1,-1))\n",
    "    action = lunar_agent.act(state, evaluation=True)\n",
    "    #env.render()\n",
    "    state, reward, done, _ = env.step(action)\n",
    "    print(reward)    \n",
    "    if done:\n",
    "        break \n",
    "#env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "import numpy as np \n",
    "\n",
    "def dqn(n_episodes=100, max_t=100, eps_start=1.0, eps_end=0.01, eps_decay=0.995):\n",
    "    scores = []                        # list containing scores from each episode\n",
    "    scores_window = deque(maxlen=100)  # last 100 scores\n",
    "    eps = eps_start                    # initialize epsilon\n",
    "\n",
    "    for i_episode in range(1, n_episodes+1):\n",
    "        print(i_episode)\n",
    "        state = env.reset()\n",
    "        state = tf.reshape(state,shape=(1,-1))\n",
    "\n",
    "        score = 0\n",
    "        for t in range(max_t):\n",
    "            action = lunar_agent.act(state, eps)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            next_state = tf.reshape(next_state,shape=(1,-1))\n",
    "            lunar_agent.step(state, action, reward, next_state, done)\n",
    "            state = next_state\n",
    "            score += reward\n",
    "            if done:\n",
    "                break \n",
    "\n",
    "        scores_window.append(score)       # save most recent score\n",
    "        scores.append(score)              # save most recent score\n",
    "        eps = max(eps_end, eps_decay*eps) # decrease epsilon\n",
    "        print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_window)), end=\"\")\n",
    "        if i_episode % 100 == 0:\n",
    "            print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_window)))\n",
    "        # if np.mean(scores_window)>=200.0:\n",
    "        #     print('\\nEnvironment solved in {:d} episodes!\\tAverage Score: {:.2f}'.format(i_episode-100, np.mean(scores_window)))\n",
    "        #     torch.save(agent.qnetwork_local.state_dict(), 'checkpoint.pth')\n",
    "        #     break\n",
    "    return scores\n",
    "dqn()"
   ]
  },
  {
   "source": [
    "## Trading Agent"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from SmartTradingBot import agent, utils, trainer\n",
    "from SmartTradingBot.utils import get_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = get_data(['BTC-USD'], start_date=\"2019-06-01\", end_date=\"2020-09-01\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "#sns.lineplot(train.index, train)\n",
    "normalised_train = utils.normalised_difference(data=train)\n",
    "signorm_train = utils.sigmoid(normalised_train)\n",
    "sns.lineplot(train.index[:-1],signorm_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trading_agent = agent.DQNAgent(\n",
    "    state_dim=10, # 10 days data is one \"state1\"/feature\n",
    "    action_dim=3, # [Hold,Buy,Sell] = [0,1,2]\n",
    "    hidden_layer_sizes=[128, 256, 256, 128],\n",
    "    buffer_size=1000,\n",
    "    batch_size=32,\n",
    "    discount=0.99,\n",
    "    learning_rate=1e-3,\n",
    "    learning_freq=4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "n_episodes = 50\n",
    "results=[]\n",
    "for episode in range(1, n_episodes):\n",
    "    trainer.train_bot(agent=trading_agent, data=signorm_train, episode=episode, n_episodes=n_episodes)\n",
    "    results.append(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}
